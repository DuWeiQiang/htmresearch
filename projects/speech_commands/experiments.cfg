[DEFAULT]

repetitions = 1
iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
weight_decay = 0.01
learning_rate_factor = 1.0
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 2000
k = 200
weight_sparsity = 0.50
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = True # If False, will only test noise at end

path = results
datadir = "data"
background_noise_dir = "_background_noise_"

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = "StepLR"

; Configure lr_scheduler class constructor using kwargs style dictionary
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"


model_type = "resnet9"  # "cnn", "resnet9", or linear
c1_out_channels = 10
c1_k = 6
dropout = 0.5

count_nonzeros = False

;
; Gets about 96.6% test error after 15 iterations. More iters might help.
;
;[resnet9]
;iterations = 15
;learning_rate_factor = 0.8
;learning_rate = 0.04
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 40
;test_noise_every_epoch = False
;batches_in_epoch = 214
;batch_size = 96
;model_type = "resnet9"

[resnetQuick]
iterations = 3
learning_rate_factor = 0.8
learning_rate = 0.04
momentum = 0.9
weight_sparsity = 1.0
dropout = 0.0
log_interval = 2
test_noise_every_epoch = False
batches_in_epoch = 10
batch_size = 16
model_type = "resnet9"

;[linear2]
;n = 1600
;k = 1600
;iterations = 15
;learning_rate_factor = 0.8
;learning_rate = 0.01
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 40
;test_noise_every_epoch = False
;batches_in_epoch = 214
;batch_size = 96
;model_type = "linear"


;[sparseLinear1]
;n = 1600
;k = 200
;k_inference_factor = 1.5
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate_factor = 0.8
;learning_rate = 0.001
;momentum = 0.0
;weight_sparsity = 0.4
;dropout = 0.0
;log_interval = 1000
;test_noise_every_epoch = False
;batches_in_epoch = 5121
;batch_size = 4
;model_type = "linear"

;[experimentQuick]
;n = 50
;k = 50
;iterations = 4
;boost_strength = 1.0
;boost_strength_factor = [0.9]
;learning_rate_factor = 0.8
;learning_rate = 0.04
;momentum = 0.9
;weight_sparsity = 1.0
;dropout = 0.0
;log_interval = 2
;test_noise_every_epoch = False
;batches_in_epoch = 10
;batch_size = 16
;
