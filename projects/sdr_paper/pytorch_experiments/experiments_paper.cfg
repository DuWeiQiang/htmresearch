#
# Parameters used in the final ICML paper
#

[DEFAULT]
# Keep final results in a separate location
path = results

experiment = grid
repetitions = 1
seed = 42
datadir = data
lr_scheduler = StepLR
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"
validation = 1.0
no_cuda = False
batches_in_epoch = 100000
log_interval = 1000
test_batch_size = 1000
create_plots = False
count_nonzeros = True
test_noise_every_epoch = True

use_batch_norm = True
boost_strength_factor = 1.0
optimizer = SGD

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the main networks described in the MNIST section
; of the paper.


;[denseCNN1]
;c1_out_channels = "30"
;c1_k = "4320"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;saveNet = True

;[denseCNN2]
;c1_out_channels = "30_30"
;c1_k = "4320_480"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 0.0
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;saveNet = True

;[sparseCNN1]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 20
;boost_strength = 1.4
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = True


;[sparseCNN2]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 300
;k = 50
;iterations = 20
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;use_batch_norm = False
;saveNet = True


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the mixed networks described in the MNIST section
; of the paper.

; Dense CNN layers with a sparse hidden layer identical to Sparse CNN2
;[denseCNN2SP3]
;c1_out_channels = "30_30"
;c1_k = "4320_480"
;n = 300
;k = 50
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.5
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;saveNet = False

; Uncomment to average over multiple seeds
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]


; Sparse CNN layers with a dense hidden layer like Dense CNN2
;[sparseCNN2D3]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False

; Uncomment to average over multiple seeds
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]


; Same as Sparse CNN-2 except the hidden layer has weight sparsity = 1
;[sparseCNN2W1]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 300
;k = 50
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False

; Uncomment to average over multiple seeds
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]


; Sparse CNN-2 with a hidden layer like Dense CNN-2 but with weight sparsity 0.3
;[sparseCNN2DSW]
;c1_out_channels = "30_30"
;c1_k = "400_400"
;n = 1000
;k = 1000
;iterations = 15
;boost_strength = 1.5
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.8
;momentum = 0.9
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;dropout = 0.0
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 16
;validation = 1.0
;use_batch_norm = False
;saveNet = False

; Uncomment to average over multiple seeds
;seed = [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]
