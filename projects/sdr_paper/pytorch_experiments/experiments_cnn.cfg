[DEFAULT]

repetitions = 1
iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.01
learning_rate_factor = 1.0
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 100
k = 100
weight_sparsity = 1.0
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = True # If False, will only test noise at end

path = results
datadir = "data"

optimizer = SGD

; CNN specific parameters
use_cnn = True
c1_out_channels = 20
c1_k = 2000000
use_dropout = False


;[standardOneLayer]
;n = [100,150]
;log_interval = 1000
;batches_in_epoch = 10000
;iterations = 14
;learning_rate_factor = 0.9
;test_noise_every_epoch = True

;Smaller batch sizes
;[bestSparseNet]
;n = [500]
;k = [50]
;boost_strength = [1.5]
;learning_rate = 0.04
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 10
;k_inference_factor = 1.5
;boost_strength_factor = 0.9
;batch_size = 4

;[cnn1]
;c1_out_channels = 30
;c1_k = 400
;n = 100
;k = 10
;iterations = 10
;boost_strength = 1.0
;boost_strength_factor = [0.9]
;learning_rate = [0.02, 0.01]
;learning_rate_factor = 0.85
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Test weight sparsity
;[cnn2]
;c1_out_channels = 30
;c1_k = 400
;n = 100
;k = 10
;iterations = 10
;boost_strength = 1.0
;boost_strength_factor = [0.9]
;learning_rate = 0.01
;learning_rate_factor = 0.85
;weight_sparsity = [1.0,0.8,0.6,0.4]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4


;[cnn3]
;c1_out_channels = 30
;c1_k = 400
;n = 100
;k = 10
;iterations = 10
;boost_strength = 1.0
;boost_strength_factor = [0.9]
;learning_rate = 0.01
;learning_rate_factor = 0.85
;weight_sparsity = [0.4, 0.3, 0.2]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; Testing learning rates
;[cnn4]
;c1_out_channels = 30
;c1_k = 400
;n = 100
;k = 10
;iterations = 15
;boost_strength = 1.0
;boost_strength_factor = [0.9]
;learning_rate = [0.01, 0.005]
;learning_rate_factor = [0.85,0.8,0.75,0.7]
;weight_sparsity = [0.4]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

;[cnn5]
;c1_out_channels = 30
;c1_k = 400
;n = 100
;k = 10
;iterations = 15
;boost_strength = [1.2,1.4]
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = [0.85,0.8]
;weight_sparsity = [0.4]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;

;[cnn6]
;c1_out_channels = [25,30,35]
;c1_k = [400,350,450]
;n = [100,150]
;k = 10
;iterations = 15
;boost_strength = 1.2
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = [0.8, 0.7]
;weight_sparsity = [0.4, 0.35, 0.3]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

; How important is weight sparsity?
;[cnn7]
;c1_out_channels = 30
;c1_k = 400
;n = [100,150]
;k = 10
;iterations = 15
;boost_strength = 1.2
;boost_strength_factor = 0.85
;learning_rate = 0.01
;learning_rate_factor = 0.7
;weight_sparsity = [1.0, 0.8, 0.6, 0.4]
;k_inference_factor = 1.5
;use_cnn = True
;use_dropout = False
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4

[cnnQuick]
c1_out_channels = 5
c1_k = 10
n = 100
k = 100
iterations = 1
boost_strength = 1.0
boost_strength_factor = [0.9]
learning_rate = 0.02
learning_rate_factor = 0.85
weight_sparsity = 0.4
k_inference_factor = 1.5
use_cnn = True
use_dropout = False
log_interval = 2000
test_noise_every_epoch = False
batches_in_epoch = 40
create_plots = False
batch_size = 4
