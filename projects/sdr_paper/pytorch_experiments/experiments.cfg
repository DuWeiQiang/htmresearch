[DEFAULT]

repetitions = 1
iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
learning_rate_factor = 1.0
use_batch_norm = True
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 2000
k = 200
weight_sparsity = 0.50
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = True # If False, will only test noise at end

; validation dataset ratio. Train on X%, validate on (1-X)%.
; Set to 1.0 to skip the validation step and use the whole training dataset
validation = float(50000.0/60000.0)

path = results
datadir = "data"

optimizer = SGD

; Learning Rate Scheduler. See "torch.optim.lr_scheduler" for valid class names
lr_scheduler = "StepLR"

; Configure lr_scheduler class constructor using kwargs style dictionary
lr_scheduler_params = "{'step_size': 1, 'gamma':%(learning_rate_factor)s}"

; CNN specific parameters
use_cnn = False
c1_out_channels = 10
c1_k = 6
dropout = 0.5

count_nonzeros = False

;[standardOneLayer]
;n = 200
;k = 200
;boost_strength = 0.0
;learning_rate = 0.01
;momentum = 0.5
;weight_sparsity = 1.0

;Best one layer sparse net as of 1/7/2019
;[bestSparseNet]
;n = [500]
;k = [50]
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.04
;learning_rate_factor = 0.5
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 7
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 1000
;dropout = 0.0


[experimentQuick]
n = 50
k = 10
iterations = 4
boost_strength = 1.0
boost_strength_factor = [0.9]
learning_rate_factor = 0.8
weight_sparsity = 0.4
k_inference_factor = 1.0
use_cnn = False
dropout = 0.0
log_interval = 2000
test_noise_every_epoch = False
batches_in_epoch = 40
create_plots = False
batch_size = 40
validation = 1.0


; See https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
;[DropoutExperimentDense]
;n = 800
;k = 800
;boost_strength = 0.0
;learning_rate = 0.01
;momentum = 0.5
;weight_sparsity = 1.0
;dropout=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

;[DropoutExperimentDenseCNN]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 150
;iterations = 9
;boost_strength = 1.4
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 1.0
;k_inference_factor = 1.5
;use_cnn = True
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;dropout=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

; parameters from [bestSparseNet]
;[DropoutExperimentSparse]
;n = 500
;k = 50
;boost_strength = 1.5
;learning_rate = 0.04
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 10
;k_inference_factor = 1.5
;boost_strength_factor = 0.9
;batch_size = 4
;dropout=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

; parameters from [bestSparseCNN]
;[DropoutExperimentSparseCNN]
;c1_out_channels = 30
;c1_k = 400
;n = 150
;k = 50
;iterations = 9
;boost_strength = 1.4
;boost_strength_factor = 0.85
;learning_rate = 0.02
;momentum = 0.0
;learning_rate_factor = 0.7
;weight_sparsity = 0.3
;k_inference_factor = 1.5
;use_cnn = True
;log_interval = 2000
;test_noise_every_epoch = True
;batches_in_epoch = 4000
;create_plots = False
;batch_size = 4
;validation = 1.0
;dropout=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
