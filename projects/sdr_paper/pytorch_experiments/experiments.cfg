[DEFAULT]

batch_size = 64         # mini batch size
test_batch_size = 1000
learning_rate = 0.02
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 2000
k = 200
weight_sparsity = 0.50
k_inference_factor = 1.0

repetitions = 1
iterations = 20         # Number of training epochs
no_cuda = False         # If True, disables CUDA training
log_interval = 1000     # how many minibatches to wait before logging
                        # training status

path = results
datadir = "projects/sdr_paper/pytorch_experiments/data"


; CNN specific parameters
use_cnn = False
c1_out_channels = 10
c1_k = 6
use_dropout = True


;[experiment1]
;n = 1000
;k = [200, 100]
;boost_strength = [0.0, 1.0, 2.0, 3.0]
;learning_rate = [0.02, 0.04, 0.06]
;momentum = [0.5, 0.25]
;
;[experiment2]
;n = 2000
;k = [300, 200, 100]
;boost_strength = [0.0, 0.5, 1.0]
;learning_rate = [0.02]
;momentum = [0.5, 0.25]

; Took too long!!
;[experiment2Stopped]
;weight_sparsity = 0.5
;no_cuda = False
;learning_rate = [0.02, 0.04, 0.06]
;repetitions = 1
;test_batch_size = 1000
;batch_size = 64
;n = 2000
;seed = 42
;log_interval = 1000
;iterations = 12
;path = results
;boost_strength = [0.0, 0.5, 1.0, 1.5, 2.0]
;k = [300, 200, 100]
;momentum = [0.5, 0.25, 0.0]

;[experiment3]
;n = 2000
;k = [400, 200, 100]
;boost_strength = [0.0, 0.5]
;learning_rate = [0.02]

;Testing the effect of weight sparsity
;[experiment4]
;n = 2000
;k = [200, 100]
;boost_strength = [0.0, 0.5, 1.0, 1.5]
;weight_sparsity = [0.80, 0.5, 0.35, 0.25]

;Test effect of higher n
;[experimentTemp]
;n = 4000
;k = [400, 200]
;boost_strength = [0.0, 0.5, 1.0, 1.5]
;weight_sparsity = [0.80, 0.5, 0.35, 0.25]

;Fixed boosting to start immediately and check impact of boost
;Stopped
;[experiment6]
;n = 1000
;k = [200, 100]
;boost_strength = [0.0, 0.5, 1.0, 1.5, 2.0]
;learning_rate = [0.04]
;momentum = [0.5, 0.25]
;weight_sparsity = [0.5, 0.35]

;Fixed boosting to start immediately and check impact of boost
;Fewer iterations
;[experiment7]
;n = 1000
;k = [200, 100]
;boost_strength = [0.0, 0.5, 1.0, 1.5, 2.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.5, 0.35]
;iterations = 10

;As above but with larger numbers
;[experiment8]
;n = 2000
;k = [200, 100]
;boost_strength = [0.0, 1.0, 1.5, 2.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.45, 0.35, 0.25]
;iterations = 20

;Copy of above for testing larger instance
;[experiment9]
;n = 2000
;k = [200, 100]
;boost_strength = [0.0, 1.0, 1.5, 2.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.45, 0.35, 0.25]
;iterations = 20

;Smaller numbers may work better, so let's try those
;So far the best
;[experiment10]
;n = [1000,750,500]
;k = [100,50]
;boost_strength = [0.0, 0.5, 1.0, 1.5]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.35, 0.4, 0.45]
;iterations = 20

;Best one from above, saving model
;[experiment10best]
;n = [500]
;k = [50]
;boost_strength = [0.5, 1.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20

;How bad is boost factor??
;[experiment11]
;n = [500]
;k = [50]
;boost_strength = [2.0, 3.0, 4.0, 6.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20

;Don't use momentum and use smaller batch sizes
;[experiment12]
;n = [500]
;k = [50]
;boost_strength = [0.0, 0.5, 1.0, 1.5]
;learning_rate = [0.04]
;momentum = [0.0]
;weight_sparsity = [0.4]
;iterations = 20
;batch_size = [8,16]


;Try some boost strength factor changes ABORTED
;[experiment13]
;n = [500]
;k = [50]
;boost_strength = [5.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20
;boost_strength_factor = [0.9, 0.8]

;Try some more boost strength factor changes
;[experiment14]
;n = [500]
;k = [50]
;boost_strength = [2.0, 3.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20
;boost_strength_factor = [0.95]

;Best one, random number changes
;[experiment15]
;n = [500]
;k = [50]
;boost_strength = [1.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20
;seed = [43, 44, 45, 46]

;Reduced dutyCyclePeriod to 1000 as in SP
;[experiment16]
;n = [500]
;k = [50]
;boost_strength = [0.5, 1.0, 1.5]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20

;Normal 1 non-sparse layer
;[experiment17]
;n = [500]
;k = [500]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [1.0, 0.5, 0.4]
;iterations = 20

;Test GPU speed
;[experiment18]
;n = [500]
;k = [50, 500]
;boost_strength = [1.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [1.0, 0.4]
;iterations = 20

; Taking 10best and trying different k inference factors
;[experiment19]
;n = [500]
;k = [50]
;boost_strength = 1.0
;learning_rate = 0.04
;momentum = 0.25
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.0, 1.25, 1.5, 2.0]

; Trying higher n
;[experiment20]
;n = [1000]
;k = [50]
;boost_strength = [1.0, 1.5]
;learning_rate = 0.04
;momentum = 0.25
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.0, 1.5, 2.0, 2.5, 3.0]

; Trying higher n
;[experiment21]
;n = [1000]
;k = [100,200]
;boost_strength = [1.0, 1.5, 2.0]
;learning_rate = 0.04
;momentum = 0.25
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.5, 2.0, 2.5, 3.0]
;boost_strength_factor = [1.0,0.9,0.8]

;Boost strength factors with larger n
;[experiment22]
;n = [1000]
;k = [100]
;boost_strength = [2.0, 2.5, 3.0, 3.5, 4.0]
;learning_rate = 0.04
;momentum = 0.25
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.5, 2.0]
;boost_strength_factor = [0.9,0.8,0.7]

;Smaller batch sizes
;[experiment23]
;n = [500]
;k = [50]
;boost_strength = [1.0, 2.0, 2.5, 3.0]
;learning_rate = [0.04,0.02]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.5, 2.0]
;boost_strength_factor = [1.0,0.9,0.8]
;batch_size = [4,8,16,32]


;Best of 23 with larger n
;[experiment24]
;n = [1000]
;k = [50]
;boost_strength = [1.5]
;learning_rate = [0.04]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.5]
;boost_strength_factor = [0.85]
;batch_size = [4]

;Best of 24, more iterations
;[experiment24Longer]
;n = [1000]
;k = [50]
;boost_strength = [1.5,2.0]
;learning_rate = [0.04]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 30
;k_inference_factor = [1.5]
;boost_strength_factor = [0.9]
;batch_size = [4]
;log_interval = 4000

;Smaller batch sizes
;[experiment23Best2]
;n = [500]
;k = [50]
;boost_strength = [1.0]
;learning_rate = [0.04]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [2.0]
;boost_strength_factor = [0.9]
;batch_size = [4]

;Batch size 1
;[experiment24]
;n = [500]
;k = [50]
;boost_strength = [1.0]
;learning_rate = [0.04,0.02]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [2.0]
;boost_strength_factor = [0.9]
;batch_size = [1]
;log_interval = 10000

[experimentQuick]
n = 50
k = [10]
iterations = 10
boost_strength = [2.0]
weight_sparsity = [0.4]
k_inference_factor = 1.0
use_cnn = True
