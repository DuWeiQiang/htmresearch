[DEFAULT]

repetitions = 1
iterations = 20             # Number of training epochs
batch_size = 64             # mini batch size
batches_in_epoch = 100000
test_batch_size = 1000

learning_rate = 0.04
learning_rate_factor = 1.0
momentum = 0.25
boost_strength = 2.0
boost_strength_factor = 1.0
seed = 42
n = 2000
k = 200
weight_sparsity = 0.50
k_inference_factor = 1.0

no_cuda = False             # If True, disables CUDA training
log_interval = 1000         # how many minibatches to wait before logging
create_plots = False
test_noise_every_epoch = True # If False, will only test noise at end

path = results
datadir = "projects/sdr_paper/pytorch_experiments/data"

optimizer = SGD

; CNN specific parameters
use_cnn = False
c1_out_channels = 10
c1_k = 6
use_dropout = True


;[standardOneLayer]
;n = 200
;k = 200
;boost_strength = 0.0
;learning_rate = 0.01
;momentum = 0.5
;weight_sparsity = 1.0

;Smaller batch sizes
;[bestSparseNet]
;n = [500]
;k = [50]
;boost_strength = [1.5]
;learning_rate = 0.04
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 10
;k_inference_factor = 1.5
;boost_strength_factor = 0.9
;batch_size = 4


;[experiment1]
;n = 1000
;k = [200, 100]
;boost_strength = [0.0, 1.0, 2.0, 3.0]
;learning_rate = [0.02, 0.04, 0.06]
;momentum = [0.5, 0.25]
;
;[experiment2]
;n = 2000
;k = [300, 200, 100]
;boost_strength = [0.0, 0.5, 1.0]
;learning_rate = [0.02]
;momentum = [0.5, 0.25]

; Took too long!!
;[experiment2Stopped]
;weight_sparsity = 0.5
;no_cuda = False
;learning_rate = [0.02, 0.04, 0.06]
;repetitions = 1
;test_batch_size = 1000
;batch_size = 64
;n = 2000
;seed = 42
;log_interval = 1000
;iterations = 12
;path = results
;boost_strength = [0.0, 0.5, 1.0, 1.5, 2.0]
;k = [300, 200, 100]
;momentum = [0.5, 0.25, 0.0]

;[experiment3]
;n = 2000
;k = [400, 200, 100]
;boost_strength = [0.0, 0.5]
;learning_rate = [0.02]

;Testing the effect of weight sparsity
;[experiment4]
;n = 2000
;k = [200, 100]
;boost_strength = [0.0, 0.5, 1.0, 1.5]
;weight_sparsity = [0.80, 0.5, 0.35, 0.25]

;Test effect of higher n
;[experimentTemp]
;n = 4000
;k = [400, 200]
;boost_strength = [0.0, 0.5, 1.0, 1.5]
;weight_sparsity = [0.80, 0.5, 0.35, 0.25]

;Fixed boosting to start immediately and check impact of boost
;Stopped
;[experiment6]
;n = 1000
;k = [200, 100]
;boost_strength = [0.0, 0.5, 1.0, 1.5, 2.0]
;learning_rate = [0.04]
;momentum = [0.5, 0.25]
;weight_sparsity = [0.5, 0.35]

;Fixed boosting to start immediately and check impact of boost
;Fewer iterations
;[experiment7]
;n = 1000
;k = [200, 100]
;boost_strength = [0.0, 0.5, 1.0, 1.5, 2.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.5, 0.35]
;iterations = 10

;As above but with larger numbers
;[experiment8]
;n = 2000
;k = [200, 100]
;boost_strength = [0.0, 1.0, 1.5, 2.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.45, 0.35, 0.25]
;iterations = 20

;Copy of above for testing larger instance
;[experiment9]
;n = 2000
;k = [200, 100]
;boost_strength = [0.0, 1.0, 1.5, 2.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.45, 0.35, 0.25]
;iterations = 20

;Smaller numbers may work better, so let's try those
;So far the best
;[experiment10]
;n = [1000,750,500]
;k = [100,50]
;boost_strength = [0.0, 0.5, 1.0, 1.5]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.35, 0.4, 0.45]
;iterations = 20

;Best one from above, saving model
;[experiment10best]
;n = [500]
;k = [50]
;boost_strength = [0.5, 1.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20

;How bad is boost factor??
;[experiment11]
;n = [500]
;k = [50]
;boost_strength = [2.0, 3.0, 4.0, 6.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20

;Don't use momentum and use smaller batch sizes
;[experiment12]
;n = [500]
;k = [50]
;boost_strength = [0.0, 0.5, 1.0, 1.5]
;learning_rate = [0.04]
;momentum = [0.0]
;weight_sparsity = [0.4]
;iterations = 20
;batch_size = [8,16]


;Try some boost strength factor changes ABORTED
;[experiment13]
;n = [500]
;k = [50]
;boost_strength = [5.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20
;boost_strength_factor = [0.9, 0.8]

;Try some more boost strength factor changes
;[experiment14]
;n = [500]
;k = [50]
;boost_strength = [2.0, 3.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20
;boost_strength_factor = [0.95]

;Best one, random number changes
;[experiment15]
;n = [500]
;k = [50]
;boost_strength = [1.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20
;seed = [43, 44, 45, 46]

;Reduced dutyCyclePeriod to 1000 as in SP
;[experiment16]
;n = [500]
;k = [50]
;boost_strength = [0.5, 1.0, 1.5]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [0.4]
;iterations = 20

;Normal 1 non-sparse layer
;[experiment17]
;n = [500]
;k = [500]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [1.0, 0.5, 0.4]
;iterations = 20

;Test GPU speed
;[experiment18]
;n = [500]
;k = [50, 500]
;boost_strength = [1.0]
;learning_rate = [0.04]
;momentum = [0.25]
;weight_sparsity = [1.0, 0.4]
;iterations = 20

; Taking 10best and trying different k inference factors
;[experiment19]
;n = [500]
;k = [50]
;boost_strength = 1.0
;learning_rate = 0.04
;momentum = 0.25
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.0, 1.25, 1.5, 2.0]

; Trying higher n
;[experiment20]
;n = [1000]
;k = [50]
;boost_strength = [1.0, 1.5]
;learning_rate = 0.04
;momentum = 0.25
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.0, 1.5, 2.0, 2.5, 3.0]

; Trying higher n
;[experiment21]
;n = [1000]
;k = [100,200]
;boost_strength = [1.0, 1.5, 2.0]
;learning_rate = 0.04
;momentum = 0.25
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.5, 2.0, 2.5, 3.0]
;boost_strength_factor = [1.0,0.9,0.8]

;Boost strength factors with larger n
;[experiment22]
;n = [1000]
;k = [100]
;boost_strength = [2.0, 2.5, 3.0, 3.5, 4.0]
;learning_rate = 0.04
;momentum = 0.25
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.5, 2.0]
;boost_strength_factor = [0.9,0.8,0.7]

;Smaller batch sizes
;[experiment23]
;n = [500]
;k = [50]
;boost_strength = [1.0, 2.0, 2.5, 3.0]
;learning_rate = [0.04,0.02]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.5, 2.0]
;boost_strength_factor = [1.0,0.9,0.8]
;batch_size = [4,8,16,32]


;Best of 23 with larger n
;[experiment24]
;n = [1000]
;k = [50]
;boost_strength = [1.5]
;learning_rate = [0.04]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [1.5]
;boost_strength_factor = [0.85]
;batch_size = [4]

;Best of 24, more iterations
;[experiment24Longer]
;n = [1000]
;k = [50]
;boost_strength = [1.5,2.0]
;learning_rate = [0.04]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 30
;k_inference_factor = [1.5]
;boost_strength_factor = [0.9]
;batch_size = [4]
;log_interval = 4000

;Smaller batch sizes
;[experiment23Best2]
;n = [500]
;k = [50]
;boost_strength = [1.0]
;learning_rate = [0.04]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [2.0]
;boost_strength_factor = [0.9]
;batch_size = [4]

;Batch size 1
;[experiment24]
;n = [500]
;k = [50]
;boost_strength = [1.0]
;learning_rate = [0.04,0.02]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 20
;k_inference_factor = [2.0]
;boost_strength_factor = [0.9]
;batch_size = [1]
;log_interval = 10000


;[experiment25]
;c1_out_channels = 30
;c1_k = 10
;n = 100
;k = [100]
;iterations = 15
;boost_strength = [2.0]
;weight_sparsity = [0.4]
;k_inference_factor = 1.0
;use_cnn = True
;log_interval = 10
;use_dropout = False


;[experiment26]
;c1_out_channels = 50
;c1_k = 15
;n = 100
;k = [100]
;iterations = 20
;boost_strength = 0.0
;weight_sparsity = [0.4]
;k_inference_factor = 1.0
;use_cnn = True
;log_interval = 50
;use_dropout = [False,True]
;learning_rate = 0.04

;[experiment27]
;c1_out_channels = 100
;c1_k = [20,15]
;n = 100
;k = [100]
;iterations = 20
;boost_strength = 0.0
;weight_sparsity = [0.4]
;k_inference_factor = 1.5
;use_cnn = True
;log_interval = 50
;use_dropout = False
;learning_rate = 0.04

;[experimentQuickCNN]
;c1_out_channels = 20
;c1_k = 5
;n = 100
;k = [100]
;iterations = 15
;boost_strength = [2.0]
;weight_sparsity = [0.4]
;k_inference_factor = 1.0
;use_cnn = True
;log_interval = 10
;use_dropout = False

;Larger networks again
;[experiment28]
;n = [2048]
;k = [40,80]
;boost_strength = [10.0]
;learning_rate = [0.02,0.01]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 5
;k_inference_factor = 1.5
;boost_strength_factor = [0.8]
;batch_size = 4
;log_interval = 2000
;use_cnn = False
;use_dropout = False
;create_plots = True

;[experiment29]
;n = [1024]
;k = [40,80]
;boost_strength = [10.0]
;learning_rate = [0.02,0.01]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 5
;k_inference_factor = 1.5
;boost_strength_factor = [0.8]
;batch_size = 4
;log_interval = 2000
;use_cnn = False
;use_dropout = False
;create_plots = True

;[experiment30]
;n = [1024]
;k = [100]
;boost_strength = [3.0]
;learning_rate = [0.005,0.001]
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 3
;k_inference_factor = 1.5
;boost_strength_factor = [0.8,0.9]
;batch_size = 4
;log_interval = 1000
;use_cnn = False
;use_dropout = False
;create_plots = True

;Test learning rate factors
;[exp31]
;n = [500]
;k = [50]
;boost_strength = [1.0,1.5]
;learning_rate = 0.04
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.4
;iterations = 10
;k_inference_factor = 1.5
;boost_strength_factor = 0.9
;batch_size = 4
;log_interval = 500

;Test learning rate factors
;[exp32]
;n = [500]
;k = [50]
;boost_strength = [1.0]
;boost_strength_factor = 0.9
;learning_rate = [0.04,0.02,0.01]
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 500
;iterations = 5

;Test learning rate factors
;[exp33]
;n = [500]
;k = [50]
;boost_strength = [1.5]
;boost_strength_factor = 0.9
;learning_rate = [0.04,0.02,0.01]
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 500
;iterations = 5

;[exp34]
;n = [750]
;k = [50]
;boost_strength = [1.2]
;boost_strength_factor = 0.9
;learning_rate = [0.03]
;learning_rate_factor = 0.8
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 500
;iterations = 4

;[exp35]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04,0.03,0.02]
;learning_rate_factor = [0.8,0.6,0.5]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 1000
;iterations = 8

;Adjust learning rate more frequently
;[exp36]
;n = 500
;k = 50
;boost_strength = 1.3
;boost_strength_factor = 0.9
;learning_rate = [0.04,0.03]
;learning_rate_factor = [0.8,0.6]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 1000
;iterations = 30
;batches_in_epoch = 2500
;test_noise_every_epoch = False

;[exp37]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04]
;learning_rate_factor = [0.9, 0.8, 0.7, 0.6]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 1000
;iterations = 15
;batches_in_epoch = 5000
;test_noise_every_epoch = False

;[exp38]
;n = 500
;k = 50
;boost_strength = 1.5
;boost_strength_factor = 0.9
;learning_rate = [0.02]
;learning_rate_factor = [0.9, 0.8, 0.7, 0.6]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 1000
;iterations = 15
;batches_in_epoch = 5000
;test_noise_every_epoch = True

;[exp39]
;n = 500
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04,0.05,0.06]
;learning_rate_factor = [0.9, 0.8, 0.7]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 1000
;iterations = 15
;batches_in_epoch = 5000
;test_noise_every_epoch = True


;[exp40]
;n = 500
;k = [50]
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04]
;learning_rate_factor = [0.9,0.8,0.7,0.6]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = [1.5]
;batch_size = 4
;log_interval = 5000
;iterations = 15
;batches_in_epoch = 5000
;test_noise_every_epoch = True

;[exp41]
;n = 500
;k = [50]
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04]
;learning_rate_factor = [1.0, 0.95, 0.9, 0.85]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = [1.5]
;batch_size = 4
;log_interval = 5000
;iterations = 15
;batches_in_epoch = 5000
;test_noise_every_epoch = True


; Test k inference factor
;[exp42]
;n = 500
;k = [50,75]
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04]
;learning_rate_factor = [0.9]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = [1.0, 1.5]
;batch_size = 4
;log_interval = 5000
;iterations = 15
;batches_in_epoch = 5000
;test_noise_every_epoch = True

; Test n
;[exp43]
;n = [600,550,500,450]
;k = 50
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = [0.04]
;learning_rate_factor = [0.9,0.85]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 5000
;iterations = 15
;batches_in_epoch = 5000
;test_noise_every_epoch = True


; Test random seeds
;[exp44]
;n = [500]
;k = [50]
;boost_strength = 1.0
;boost_strength_factor = 0.9
;learning_rate = 0.04
;learning_rate_factor = [0.85]
;momentum = 0.0
;weight_sparsity = 0.4
;k_inference_factor = 1.5
;batch_size = 4
;log_interval = 5000
;iterations = 15
;batches_in_epoch = 5000
;test_noise_every_epoch = True
;repetitions = 6


;[experimentQuick]
;n = 50
;k = 10
;iterations = 2
;boost_strength = 2.0
;boost_strength_factor = [0.9]
;learning_rate_factor = 0.8
;weight_sparsity = 0.4
;k_inference_factor = 1.0
;use_cnn = False
;use_dropout = False
;log_interval = 100
;test_noise_every_epoch = False
;batches_in_epoch = 100
;create_plots = False
;repetitions = 4

[experimentQuickCNN]
c1_out_channels = 6
c1_k = 10
n = 50
k = 10
iterations = 4
boost_strength = 2.0
boost_strength_factor = [0.9]
learning_rate_factor = 0.8
weight_sparsity = 0.4
k_inference_factor = 1.0
use_cnn = True
use_dropout = False
log_interval = 100
test_noise_every_epoch = False
batches_in_epoch = 100000
create_plots = False
